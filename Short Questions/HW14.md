# Homework 14 — Apache Kafka
**Author: M. Yang**

## Question 1
> Explain the following concepts and how they coordinate with each other.
> - Broker
> - ZooKeeper
> - Topic
> - Partition
> - Offset
> - Producer
> - Consumer Group

### ◆ Broker
- A **broker** is a Kafka server that stores data and handles client requests (producing and consuming).
- Each broker can host multiple partitions of different topics.
- Multiple brokers form a Kafka **cluster** for fault tolerance and load balancing.

### ◆ Zookeeper
- **Zookeeper** is used (in older Kafka versions) to coordinate Kafka cluster state and metadata.
- Responsibilities include:
    - Broker discovery
    - Controller election
    - Cluster configuration
- Kafka is transitioning to **KRaft mode** to eliminate the need for Zookeeper.

### ◆ Topic
- A **topic** is a logical channel to which producers send messages and from which consumers read messages.
- It acts like a message queue or feed name.
- Each topic can have multiple **partitions**
- Topics are **append-only** logs (existing messages in the topic are not modified or deleted unless by retention policies).

### ◆ Partition
- A **partition** is a unit of parallelism within a topic.
- Messages in a partition are ordered and identified by a unique **offset**.
- Partitions enable **horizontal scalability** and **parallel processing**.

### ◆ Offset
- An **offset** is a unique identifier for each record within a partition.
- Offsets mark the position of messages and are used by consumers to track progress.
- Consumers can **commit** offsets to remember their last read position.

### ◆ Producer
- A **producer** sends records (messages) to Kafka topics.
- It can choose which partition to write to (e.g., round-robin, based on key).
- Producers are optimized for **high throughput** and can be asynchronous.

### ◆ Consumer Group
- A **consumer group** is a group of consumers working together to consume data from a topic.
- Kafka ensures that each partition is consumed by only **one consumer** in a group.
- Enables **parallel processing** of partitions across consumers.

### How They Work Together

1. **Producers** publish messages to **topics** (which are divided into **partitions**).
2. **Brokers** store partitioned data and serve producer/consumer requests.
3. **Consumers** in a **consumer group** read messages from partitions.
4. **Offsets** help consumers track which messages have been processed.
5. **Zookeeper** (or KRaft in newer versions) manages cluster coordination and metadata.


## Question 2
> Given $P$ partitions and $C$ consumers, what happens when
> 1. $P$ >= $C$
> 2. $P$ < $C$

| Condition | Consumer Usage            | Partition Usage | Parallelism Limit |
|-----------|---------------------------|-----------------|-------------------|
| $P$ ≥ $C$ | All consumers active      | All partitions  | Limited by $C$    |
| $P$ < $C$ | Only $P$ consumers active | All partitions  | Limited by $P$    |


## Question 3
> Broker & Topic in Depth

### Broker Responsibilities with Topics

#### Partition Storage
- Each broker is responsible for storing one or more partitions of a topic.

#### Leader and Follower Roles
- Each partition has a leader broker and optionally one or more follower brokers (for replication).
- **Only** the **leader** handles all **read and write** requests for the partition. Followers replicate data from the leader.

#### Data Replication
- Kafka ensures fault tolerance by replicating partition data across brokers.

#### Client Interaction
- Producers send messages to the topic, which Kafka routes to the appropriate partition.
- Consumers read messages from partitions. Kafka brokers serve the data depending on partition leadership.

### Broker Coordination
Brokers communicate via Apache ZooKeeper (or KRaft mode in newer Kafka versions) to:
- **Elect** leaders for partitions.
- Maintain **metadata** about topics, partitions, and cluster state.

**Example**

```mermaid
graph TD
    style topic fill:#F4A300,stroke:#F4A300,stroke-width:2px
    style P0L fill:#8BC34A,stroke:#8BC34A,stroke-width:2px
    style P1L fill:#8BC34A,stroke:#8BC34A,stroke-width:2px
    style P2L fill:#8BC34A,stroke:#8BC34A,stroke-width:2px
    style P0R fill:#03A9F4,stroke:#03A9F4,stroke-width:2px
    style P1R fill:#03A9F4,stroke:#03A9F4,stroke-width:2px
    style P2R fill:#03A9F4,stroke:#03A9F4,stroke-width:2px
    
    subgraph Broker 2
        P0L["Partition 0 (Leader)"]
        P1R["Partition 1 (Replica)"]
    end

    subgraph Broker 1
        P1L["Partition 1 (Leader)"]
        P2R["Partition 2 (Replica)"]
    end

    subgraph Broker 0
        P2L["Partition 2 (Leader)"]
        P0R["Partition 0 (Replica)"]
    end

    topic["Topic: user-logins"]
    topic --> P0L
    topic --> P1L
    topic --> P2L

    P0L .-> P0R
    P1L .-> P1R
    P2L .-> P2R
```
※ Topic `user-logins` with 3 partitions and replication factor 2.


## Question 4
> Message Consumption

- In Kafka, consumers **pull** messages from topics rather than messages being pushed to consumers.
- This model provides **flexibility**, as consumers can:
  - Control the rate of consumption.
  - Re-read messages by adjusting offsets.
  - Pause or slow down consumption to handle backpressure.
- Kafka brokers remain stateless regarding consumer state, making the system more scalable and fault-tolerant.


## Question 5
> Exactly Once Processing

- Use exactly once semantics (EOS)
   - For **Kafka-to-external-systems** workflows, enable producer **idempotence** and use **transactional** APIs.
    ```properties
    enable.idempotence=true
    transactional.id=your-transactional-id
    ```
   
    ```java
    // Java code
    producer.initTransactions();
    producer.beginTransaction();
    producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
    producer.commitTransaction(); // or abortTransaction() on failure
    ```
   
   - For **Kafka-to-Kafka** workflows, use Kafka Streams.
    ```properties
    processing.guarantee=exactly_once_v2
    ```
   
- Only commit offsets after successful message processing.
    ```properties
    enable.auto.commit=false
    ```
    
    ```java
    consumer.commitSync();
    ```


## Question 6
> What will happen if some consumers in a consumer group are down?
> 
> Will data loss occur? Why?

In this case, typically no data loss occurs.

- **Message Retention:** Messages in Kafka are retained for a configurable period, ensuring availability even when consumers are down.
- **Rebalancing:** When a consumer fails, Kafka reassigns its partitions to other active consumers in the group, maintaining uninterrupted consumption.
- **Consumer Offset Tracking:** Kafka tracks the last processed message's offset for each consumer, allowing recovery and preventing data loss if a consumer fails.


## Question 7
> What happens if an entire consumer group is down?
> 
> Will data loss occur? Why?

In this case, data loss may occur.

- **Expired data:** If the consumer group is down for too long, the retention window may expire, leading to potential data loss if messages are deleted from Kafka before being consumed.
- **Effect on Offsets:** Kafka preserves offsets for a downed consumer group, allowing it to resume from the last committed offset upon recovery. However, if unconsumed messages are purged due to retention expiration, **data gaps** may occur.


## Question 8
> Consumer Lag & Solutions

Consumer lag is the difference between the offset of the **last message produced to a partition** and the offset of the **last message consumed by the consumer**.

### Causes of Consumer Lag
- **Slow Consumer:** The consumer application processes messages slowly due to **inefficient processing logic**, **heavy load**, or **resource constraints** (e.g., CPU, memory).
- **Consumer Group Rebalancing:** When a consumer group undergoes rebalancing, it can temporarily stop consuming messages, causing lag.
- **Network Issues:** Poor network performance can delay the consumption of messages, leading to lag.

### Solutions
- **Scale Consumers**
  - **Increase the number of consumers** in the group (up to the number of partitions)
  - **Scale vertically** by giving consumers more resources (CPU, memory)


- **Optimize Consumer Performance**
  - **Batch processing**: Process messages in batches rather than one at a time
  - **Parallel processing**: Use multi-threading within consumers
  - **Optimize processing logic**: Reduce CPU-intensive operations


- **Tune Configuration**
  - Increase `fetch.min.bytes` and `fetch.max.wait.ms` to get larger batches
  - Adjust `max.poll.records` to control how many records are returned in each poll
  - Increase `session.timeout.ms` and `heartbeat.interval.ms` if rebalances are frequent


- **Partition Management**
  - **Add more partitions** to allow for greater parallelism (requires care)
  - Ensure **even distribution** of messages across partitions


- **Handle Backpressure**
  - Implement **circuit breakers** to slow down producers if needed
  - Use **dead letter queues** for messages that repeatedly fail processing


- **Monitor Consumer Lag**
  - Use monitoring tools like Kafka's JMX metrics, or tools such as Confluent Control Center, Prometheus, Kafka Manager
  - Set up alerts when consumer lag exceeds a threshold, allowing proactive resolution before it impacts system performance


## Question 9
> Message Delivery Tracking

### Offsets
Offsets are monotonically increasing identifiers assigned to each message in a partition, allowing consumers to resume reading from the correct position and track processed messages.

### Consumer Offset Commit
- **Automatic offset commit:** Kafka commits the offset periodically without consumer intervention.
- **Manual offset commit:** The consumer explicitly commits the offset after processing a message, providing more control over message delivery.

### Producer Acknowledgement
Kafka producers can configure the acknowledgment behavior through the acks parameter, which determines how many brokers must acknowledge the receipt of a message before the producer considers it successfully written. The options are:
- `acks=0`: The producer does not wait for any acknowledgment from the broker.
- `acks=1`: The producer waits for the leader broker to acknowledge the message.
- `acks=all`: The producer waits for all in-sync replicas to acknowledge the message.


## Question 10
> Messaging Frameworks vs. Database (for Streaming)
>
> Kafka vs RabbitMQ

### Messaging Frameworks vs. Database (for Streaming)

| **Feature**                  | **Messaging Framework (Kafka, RabbitMQ, etc.)**       | **Relational Database (MySQL, etc.)**                               |
|------------------------------|-------------------------------------------------------|---------------------------------------------------------------------|
| **Throughput**               | High (optimized for large volumes of messages)        | Moderate to Low (not optimized for frequent writes/reads)           |
| **Latency**                  | Low (real-time or near real-time delivery)            | Higher (due to transaction and indexing overhead)                   |
| **Message Ordering**         | Guaranteed (with partitions or queues)                | Must be implemented manually (using timestamps or sequence numbers) |
| **Scalability**              | Horizontal (partitioned topics, consumers)            | Vertical or limited horizontal (sharding required)                  |
| **Concurrency Support**      | Built-in support for multiple producers/consumers     | Possible, but can lead to contention and locking issues             |
| **Message Acknowledgment**   | Built-in (ACK/NACK, retries, dead-letter queues)      | Manual implementation required                                      |
| **Durability Options**       | Configurable (in-memory, disk-based, replicated logs) | Durable storage by default                                          |
| **Retry Mechanism**          | Automatic retries with back-off, DLQs                 | Manual implementation needed                                        |
| **Message TTL / Expiration** | Native support                                        | Requires manual logic (e.g., cron job + timestamp column)           |
| **Backpressure Handling**    | Built-in support                                      | Must be manually managed                                            |

### Kafka vs. RabbitMQ

| Feature           | **Kafka**                              | **RabbitMQ**                             |
|-------------------|----------------------------------------|------------------------------------------|
| Model             | Log-based                              | Queue-based                              |
| Message Retention | Persistent (configurable), Re-readable | Deleted after consumption                |
| Throughput        | Very high                              | Moderate to high                         |
| Latency           | Low (optimized for throughput)         | Low (optimized for responsiveness)       |
| Scalability       | Excellent (partitioned, distributed)   | Good (clustered, but less flexible)      |
| Protocol          | Kafka protocol                         | AMQP (also STOMP, MQTT, etc.)            |
| Message Ordering  | Within partition                       | Within queue                             |
| Setup Complexity  | Higher (Kafka + ZooKeeper/KRaft)       | Lower (standalone or clustered)          |
| Use Cases         | Event streaming, analytics, logging    | Task queues, RPC, microservice messaging |


## Question 11
> With [this repository](https://github.com/CTYue/Spring-Producer-Consumer):
> 1. Set up 3 consumers in a single consumer group and send a message from Postman.
> 2. Increase the number of consumers in a single consumer group. Observe what happens.
> 3. Create multiple consumer groups and set up different numbers of consumers in each group. Observe consumer offsets.
> 4. Demo different message delivery guarantees in Kafka.
> 5. Implement a custom partition logic.

Code: [kafka-demo](../Projects/kafka-demo).

### 1. Three Consumers in a Group

<details>
<summary>Screenshots</summary>

- **Active Consumers**
  
    ![3 consumers log](https://github.com/user-attachments/assets/1fab7e79-b9af-40bc-8346-df19e20fb3f9)
    ![3 consumers UI](https://github.com/user-attachments/assets/3e8ca9fc-6f23-4650-9244-60d582fec3a6)

- **Message Consumption**
  
    ![3 consumers message log](https://github.com/user-attachments/assets/765974f2-9318-49d9-9cfc-577a0c401f2c)
    ![3 consumers message postman](https://github.com/user-attachments/assets/d29b4f19-2654-4b05-b278-3802e0da5a1e)
    ![3 consumers message UI](https://github.com/user-attachments/assets/0c1cc859-a318-4523-be28-46ac889433df)

</details>


### 2. Five Consumers in a Group

<details>
<summary>Screenshots</summary>

- **Only 3** consumers out of 5 are **active**.

  ![5 consumers log](https://github.com/user-attachments/assets/f2950a45-edd2-4262-bc03-32d7203e9735)
  ![5 consumers UI](https://github.com/user-attachments/assets/07a1114e-4fd5-4886-b44d-e934e4459360)

</details>


### 3. Two Consumer Groups
- `consumer_group_1` has 4 consumers
- `consumer_group_2` has 2 consumers

<details>
<summary>Screenshots</summary>

- **Active Consumer Groups**

  ![2 consumer groups log](https://github.com/user-attachments/assets/d6f27be0-fad6-40b0-82b8-5728bdd4e140)
  ![2 consumer groups UI](https://github.com/user-attachments/assets/4160d4ae-4085-4edf-87b2-d3cef636f5d7)

  ![consumer group 1 UI](https://github.com/user-attachments/assets/db3524d7-4049-432c-90ed-980d4ad42222)
  ![consumer group 2 UI](https://github.com/user-attachments/assets/3dad9d43-3514-4d8f-a872-21098a355516)

- **Message Consumption**

  ![2 consumer groups message log](https://github.com/user-attachments/assets/461fe049-b1af-4802-ad1b-2cd405e85151)
  ![2 consumer groups message postman](https://github.com/user-attachments/assets/748a7701-e9fc-495e-b110-36667f7881e7)
  ![2 consumer groups message UI](https://github.com/user-attachments/assets/36438597-6b8f-4cf8-9867-b5f27581c2f2)

</details>


### 4. Different Message Delivery Guarantees

<details>
<summary>Screenshots</summary>

- **At Least Once**

  ![ALO log](https://github.com/user-attachments/assets/00178d81-cab8-4c93-bfab-b88d800c7718)
  ![ALO UI](https://github.com/user-attachments/assets/b366e24d-b553-47d0-b0cc-4c8e121f812a)

- **At Most Once**

  ![AMO log](https://github.com/user-attachments/assets/6669b5f6-3803-4b72-b4d3-42235336aef1)
  ![AMO UI](https://github.com/user-attachments/assets/c95329e5-4401-4bdc-81a0-f996c8014f69)

- **Exactly Once**

  ![EO log](https://github.com/user-attachments/assets/6ab5e951-2366-426d-aa21-4428ad4fbd43)
  ![EO UI](https://github.com/user-attachments/assets/e2a63eb7-e75e-4439-8f8d-613f39dfed97)

</details>


### 5. Partition Logic

<details>
<summary>Screenshots</summary>

**Custom** Partition Logic
- For **numeric** keys: Use modulo partitioning (key `%` num_partitions)
- For **non-numeric** keys: Use Kafka's `murmur2` **hash** partitioning

![custom partition log](https://github.com/user-attachments/assets/abcf2af4-2b85-472c-9007-a36a203ae7c1)
![custom partition UI](https://github.com/user-attachments/assets/7ac75b48-9824-4509-91c2-85fcae711fad)


</details>
