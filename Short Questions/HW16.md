# Homework 16 — Apache Cassandra
**Author: M. Yang**

## Question 1
> What is the primary use case for Apache Cassandra?

Handling large volumes of write-intensive workloads.

## Question 2
> What does the partition key in Cassandra determine?

1. [x] **The node on which data is stored**
2. [ ] The sort order of rows within a partition—clustering key (clustering column)
3. [ ] The number of replicas for each piece of data—replication factor
4. [ ] The encryption method used for the data—security settings

## Question 3
> Which consistency level ensures that a write operation is acknowledged
> only after being written to the commit log and memory table of all replicas?

1. [x] **ALL**
2. [ ] QUORUM—majority (more than half)
3. [ ] ONE—one replica
4. [ ] ANY—at least one node (not necessarily replica)

## Question 4
> What does the ALLOW FILTERING keyword in CQL allow you to do?

1. [x] **Execute queries on non-primary key columns, potentially scanning multiple partitions**
2. [ ] Perform a join operation between two tables—Cassandra doesn't support joins

## Question 5
> Which of the following is a potential drawback of using ALLOW FILTERING in a query?

It can significantly impact read performance by causing full table scans.

## Question 6
> What does the TTL (Time To Live) option do in a CQL insert statement?

Specify the time duration after which the data will expire and be deleted (compaction process).

```cassandraql
INSERT INTO users (id, name) VALUES (1, 'Alice') USING TTL 3600;
```

## Question 7
> Which of the following data types can be used as a primary key in Cassandra?

1. [x] UUID
2. [x] varchar
3. [x] Text
4. [x] boolean
5. [x] decimal

## Question 8
> Given the following table schema, identify the composite partition key:
> ```cassandraql
> CREATE TABLE employee_activity (
> employee_id uuid,
> department_id uuid,
> activity_id timeuuid,
> activity_time timestamp,
> activity_description text,
> PRIMARY KEY ((employee_id, department_id), activity_id)
> );
> ```

`employee_id, department_id`

## Question 9
> What is the primary advantage of using a composite partition key in Cassandra?

1. [x] **It helps to distribute data more evenly across the cluster**
2. [ ] It simplifies the data model—Composite keys often make the model more complex.

## Question 10
> What is the purpose of the Gossip protocol in Cassandra?

1. [x] **To allow nodes to exchange state information about themselves and other nodes**
2. [ ] To manage the distribution of data across nodes—Data distribution is done by the partitioner and replication strategy, not Gossip.
3. [ ] To build up a primary-secondary backup architecture—Cassandra uses a masterless architecture, not primary-secondary.

## Question 11
> What is denormalization in the context of Cassandra?

In the context of relational database:
- normalization usually leads to dividing a database into smaller tables to reduce redundancy
- denormalization usually leads to merging data into fewer tables (to optimize read performance)

In Cassandra, these concepts may be slightly different. They're not about having more or fewer tables.
Denormalization in Cassandra is primarily about avoiding joins, which Cassandra doesn’t support efficiently.
To do this, data merging is needed to achieve the "join" effect.
However, to achieve different "join" effects, data may be duplicated for different merges.

## Question 12
> In a Cassandra cluster, what could be a possible reason for uneven load distribution (i.e., some nodes are handling significantly more read and write requests than others, causing an imbalance), and how can it be resolved?

1. [x] **Poor partition key choice causing hot-spotting; resolve by designing a better partition key**
2. [ ] Inefficient use of secondary indexes; resolve by removing unnecessary indexes—May impact query performance
3. [ ] Network latency issues; resolved by optimizing network configurations—Could cause slow responses
4. [ ] Incorrect configuration of the replication factor; resolve by adjusting the replication factor—Affects data redundancy and availability

## Question 13
> How does Cassandra’s write path work?

1. The write is first recorded in the **commit log** for durability (to recover in case of failure).
2. The data is then written to an in-memory structure called the **memtable**.
3. Later, when the memtable fills up, it is flushed to disk as an SSTable.
4. This design ensures fast writes and durability.

## Question 14
> What is a "hinted handoff" in Cassandra?

* Hinted handoff is a mechanism in Cassandra to improve **availability** and **eventual consistency**.
* If a replica node is down or unreachable when a write occurs, another node temporarily stores a hint about the missed write.
* When the downed node comes back online, the hint is replayed to the node to bring it up to date with the missed writes.

## Question 15
> Is Cassandra designed to handle high write throughput with eventual consistency?

Yes.

## Question 16
> In Cassandra, should secondary indexes be used frequently to optimize read performance?

No.

Secondary indexes may lead to inefficient queries that scan multiple nodes, causing increased latency and cluster load.
The preferred approach is to model data with **appropriate primary keys** and **denormalization** to optimize read performance.

## Question 17
> Does Cassandra support ACID transactions across multiple rows and tables?

No.

Cassandra doesn't support ACID transactions across multiple rows in different partitions or tables.
It provides atomicity (A) only at the single partition level.

## Question 18
> Cassandra's architecture makes it easy to scale horizontally by adding more nodes to the cluster.

True.

## Question 19
> Which one below is responsible for coordinating read and write operations in Cassandra?

1. [x] **Coordinator**
2. [ ] Cassandra Daemon—It's the background process (`cassandra` service), but not a specific functional role.

## Question 20
> Which of the following consistency levels provides the fastest read and write operations in Cassandra?

1. [x] **ONE—Fastest read**
2. [x] **ANY—Fastest write**
3. [ ] TWO
4. [ ] THREE
5. [ ] ALL
6. [ ] QUORUM

## Question 21
> Which file contains the configuration settings for a Cassandra node?

`cassandra.yaml`

## Question 22
> In Cassandra, what is an "SSTable"?

A **disk-based**, **immutable** file where Cassandra stores data on disk.
It's used during reads, along with indexes, bloom filters, and partition summaries.

## Question 23
> What mechanism does Cassandra use to ensure data is not lost during node failure?

Commit log.

## Question 24
> What replication strategies does Cassandra have?

1. `SimpleStrategy`:
   - Best for single data center setups.
   - Places replicas on the next N nodes in the ring.

2. `NetworkTopologyStrategy`:
   - Designed for multi-data center deployments.
   - Allows custom replication factor per data center.

## Question 25
> What is a memtable in Cassandra?

An in-memory data structure where writes are initially stored.

## Question 26
> To efficiently query the latest data from each sensor and maintain good write performance, which partition key should be used?
> ```cassandraql
> CREATE TABLE sensor_data (
> sensor_id uuid,
> timestamp timeuuid,
> value double,
> PRIMARY KEY (sensor_id, timestamp)
> ) WITH CLUSTERING ORDER BY (timestamp DESC);
> ```

`sensor_id`

## Question 27
> When working with a Cassandra table that has a lot of tombstones (deleted records),
> how to mitigate the impact of the tombstones on read performance?

1. Increasing the compaction throughput
   - Compaction merges SSTables and removes expired tombstones.
   - Increasing throughput speeds up this process, reducing tombstone accumulation.

2. Reducing the tombstone TTL (Time-to-Live)
   - Tombstones remain in the database until their TTL expires.
   - Lowering TTL causes tombstones to expire sooner, reducing their read-time impact.

3. Using leveled compaction strategy
   - Leveled Compaction Strategy (LCS) performs more frequent compactions focused on smaller SSTables.
   - This helps remove tombstones more efficiently compared to Size-Tiered Compaction Strategy (STCS).

## Question 28
> In a Cassandra cluster, data is evenly distributed across all nodes by default, regardless of the partitioner used.
>
> True or false?

False.

A poor partition key (low cardinality or skewed values) can cause hotspots, resulting in uneven load and data distribution.

## Question 29
> Design a Cassandra schema for an e-commerce application that supports:
> 1. Fast access to the latest orders for each user
> 2. Efficient retrieval of all orders for a given product

Cassandra, it’s best to use "denormalization" by creating two **separate tables tailored for each query pattern**, since Cassandra does not support joins.

1. Table for latest orders per user

    ```cassandraql
    CREATE TABLE user_orders (
        user_id uuid,
        order_id timeuuid,
        product_id uuid,
        order_date timestamp,
        order_status text,
        total_amount decimal,
        PRIMARY KEY (user_id, order_id)
    ) WITH CLUSTERING ORDER BY (order_id DESC);
    
    -- Partition key: `user_id`
    -- Clustering key: `order_id`
    ```

2. Table for all orders per product

    ```cassandraql
    CREATE TABLE product_orders (
        product_id uuid,
        order_id timeuuid,
        user_id uuid,
        order_date timestamp,
        order_status text,
        total_amount decimal,
        PRIMARY KEY (product_id, order_id)
    ) WITH CLUSTERING ORDER BY (order_id DESC);
       
    -- Partition key: `product_id`
    -- Clustering key: `order_id`
    ```


## Question 30
> SQL/OLTP (Online Transaction Processing) vs. NoSQL vs. OLAP (Online Analytical Processing)

| Feature / Criteria     | PostgreSQL (SQL)               | Cassandra / CosmosDB (NoSQL)            | Druid (OLAP)                         |
|------------------------|--------------------------------|-----------------------------------------|--------------------------------------|
| Data Model             | Relational                     | Wide-column                             | Columnar (aggregate-optimized)       |
| Use Case               | OLTP, joins, complex queries   | High-write throughput, Key-Value access | Analytics, dashboards, event streams |
| Read Latency           | Low–Moderate                   | Low (good partitioning)                 | Very low (sub-second aggregates)     |
| Write Latency          | Moderate                       | Very high scalability                   | Moderate–High                        |
| Horizontal Scalability | Limited                        | High                                    | High                                 |
| Query Support          | Full SQL                       | Limited (no joins, few filters)         | SQL-like / JSON queries              |
| Timeseries / Streaming | Limited                        | Possible, not ideal                     | Excellent                            |
| ACID Compliance        | Strong                         | Tunable (eventual/strong)               | Eventually consistent                |
| Best For               | Transactional APIs, relational | Real-time ingestion, distributed        | Dashboards, trend analytics          |
| Cloud Support          | CloudSQL, RDS                  | Cosmos DB (Azure), self-managed         | Druid on GCP                         |

| Scenario                                  | Recommended DB       | Why                                                |
|-------------------------------------------|----------------------|----------------------------------------------------|
| Transactional APIs (create/update/delete) | PostgreSQL           | ACID, strong schema, relational integrity          |
| Complex joins & business rules            | PostgreSQL           | Rich SQL support                                   |
| Distributed, globally available API       | CosmosDB             | Global distribution, multi-region support          |
| High-ingest event/log APIs                | Cassandra / CosmosDB | Horizontal scaling, high write throughput          |
| Analytics APIs (filters, aggregations)    | Druid                | Fast OLAP queries, time-series optimized           |
| Drill-down dashboards / BI tool backing   | Druid                | Near real-time aggregation, sub-second performance |
