# Homework 15 — Kubernetes
**Author: M. Yang**

## Question 1
> K8s Infrastructure

![kubernetes-cluster-architecture](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

### Control Plane (Master Nodes)

#### ◆ API Server
Services **REST operations** and provides the **frontend** to the cluster's shared state through which **all other components interact**.

#### ◆ etcd
A **distributed**, consistent **key-value store** to store all cluster **configuration** and **state** data.
`etcd` ≈ `/etc` for `d`istributed systems.

Stores [**API Objects**](#question-2) (**YAML**/JSON):
- Ingresses
- Namespaces
- Deployments
- ReplicaSets
- Pods
- Services
- PVCs
- PVs
- Secret
- ConfigMap

#### ◆ Controller Manager
Runs background controllers to **watch** cluster state and make changes to **maintain** the desired configuration.

Watches:
- Namespace creation/deletion
- Deployments → creates ReplicaSets
- ReplicaSets → creates Pods
- PVCs → binds to PVs

#### ◆ Scheduler
**Assigns** newly created **pods to nodes** based on resource availability, policies, and constraints to ensure **optimal workload** placement.


### Worker Nodes

#### ◆ Kubelet
An **agent** that **communicates** with the **API server**, and **ensures** that the **containers** described in PodSpecs (received from the API server) are **running** and remain healthy.

#### ◆ Container Runtime
The software responsible for running containers (e.g., **Docker**).

Kubelet talks to the container runtime through the container runtime interface (**CRI**). A container runtime must implement CRI to work with Kubernetes.

#### ◆ Kube-proxy
Manages **network rules** on each node and handles **internal routing** between services and pods, providing basic **load balancing**.


## Question 2
> K8s Primitives

| **Component**           | **Type**                                  | **Description**                                                                                                                                                                                                |
|-------------------------|-------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Namespace               | API Object                                | Isolates resources **logically** (e.g., by service, by dev/test/prod)                                                                                                                                          |
| Ingress                 | API Object                                | Defines **external HTTP/HTTPS routing rules**                                                                                                                                                                  |
| Ingress Controller      | Deployment (**Pod/Pods** on Worker Nodes) | Listens to `Ingress` resources and sets up **routing/load-balancing** (e.g., Nginx), **installation needed**                                                                                                   |
| Service                 | API Object                                | Exposes a **group of pods** via networking (e.g., LoadBalancer, NodePort, ClusterIP)                                                                                                                           |
| Deployment              | API Object                                | Manages **stateless** application pods (i.e., app does not store data locally; safe to restart or scale)                                                                                                       |
| ReplicaSet              | API Object                                | Ensures a certain number of **pod replicas**                                                                                                                                                                   |
| StatefulSet             | API Object                                | Manages **stateful** applications (e.g., MySQL, Cassandra, Kafka, ZooKeeper); provides stable **network identity** (e.g., `app-0`, `app-1`), **ordered** startup/shutdown, and persistent storage for each pod |
| Pod                     | API Object                                | **Smallest deployable unit**, defines a container or group of containers (e.g., app container & side-car containers)                                                                                           |
| Persistent Volume Claim | API Object                                | **User’s request** for storage (e.g., required size, access mode, storage class)                                                                                                                               |
| Persistent Volume       | API Object                                | Defines storage **resource** in the cluster (e.g., capacity, access mode, reclaim policy, storage class, volume handle)                                                                                        |
| StorageClass            | API Object                                | Defines **how** storage is **dynamically provisioned** (e.g., provisioner, parameters, reclaim policy). Specifies the backend (on-premises/cloud) and behavior for PVs created via PVCs                        |
| Secret                  | API Object                                | Stores sensitive data (e.g., JWT secret key)                                                                                                                                                                   |
| ConfigMap               | API Object                                | Stores non-sensitive app config data (e.g., `application.properties`)                                                                                                                                          |
| Container Registry      | External System                           | Where the kubelet pulls container images from (e.g., **Docker Hub**)                                                                                                                                           |


## Question 3
> Service Type:
> - ClusterIP
> - NodePort
> - LoadBalancer

| **Service Type**  | **ClusterIP**                                                                  | **NodePort**                                                 | **LoadBalancer**                                                            |
|-------------------|--------------------------------------------------------------------------------|--------------------------------------------------------------|-----------------------------------------------------------------------------|
| **Accessibility** | Internal to cluster only                                                       | External via **NodeIP:NodePort** (LAN/VPC)                   | External via **PublicIP** (Internet or VPC)                                 |
| **Routing**       | ClusterIP:port (or **ServiceName**:port)<br>→ kube-proxy<br>→ PodIP:targetPort | NodeIP:nodePort<br>→ kube-proxy<br>→ ClusterIP:port<br>→ ... | PublicIP:80 (or 443)<br>→ Cloud Load Balancer<br>→ NodeIP:nodePort<br>→ ... |
| **Use Case**      | Internal communication between **pods/services**                               | Simple external access (**dev/test**)                        | Production-ready external access                                            |
| **Notes**         | **Default** service type, **virtual** IP & port                                | **Static** `nodePort` on **each node** (30000–32767)         | Requires **external cloud LB** (e.g., AWS ELB)                              |


## Question 4
> Ways to Expose Services

| **Method** | **NodePort**                             | **LoadBalancer**                   | **Ingress**                         |
|------------|------------------------------------------|------------------------------------|-------------------------------------|
| **Notes**  | External clients on the **same network** | 1 service : 1 **public** IP/domain | N services : 1 **public** IP/domain |

### Load Balancer
![K8s-Components-LoadBalancer](https://github.com/user-attachments/assets/2d208e10-789f-47b5-becc-1b9e71394bc4)

### Ingress
![K8s-Components-Ingress](https://github.com/user-attachments/assets/b4ca7e7c-2177-4f3f-bd93-d42dcbfe7273)


## Question 5
> Auto-scaling

### Horizontal Pod Autoscaler (HPA)
Scales the number of **pod** replicas in a Deployment based on **observed metrics** (e.g., CPU, memory, custom metrics).
- Define **resource requests/limits** in the **Deployment** object.
- Define **resource thresholds** in the **HPA** object.

#### HPA Example
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
      - name: my-app
        image: my-app:latest
        resources:
          requests: # baseline guarantee (used for HPA % calculation)
            cpu: "200m" # m → millicore (0.2 core)
            memory: "256Mi" # Mi → mebibyte
          limits: # upper bound (not used by HPA)
            cpu: "500m"
            memory: "512Mi"
```

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu # based on baseline requests.cpu
      target:
        type: Utilization
        averageUtilization: 70 # average CPU usage across pods > 70% → scale out
```
**Note**
- If `Deployment.replicas` < `HPA.minReplicas`, HPA will scale up **immediately** to `minReplicas`.
- If `Deployment.replicas` > `HPA.maxReplicas`, HPA **may** scale down to `maxReplicas` once metrics are checked.
- Recommended pattern: `Deployment.replicas = HPA.minReplicas`

### Cluster Autoscaler (on Cloud)
Scales the number of **nodes** in the cluster up or down based on pod resource requests and scheduling needs.

#### Cloud CA Example
- Cluster: 2 nodes, each 2 CPU cores.
- Each node is running pods consuming 2 CPU cores (fully utilized).
- A new pod requests 0.5 CPU → scheduler cannot place it → pod is **unschedulable**.
- Cluster Autoscaler detects this → adds a new node with 2 CPU cores → pod is scheduled on the new node.


## Question 6
> Handling Node Failures

### 1. Node Health Monitoring
- Each node runs a **kubelet** process that **reports** the node’s status (Ready, NotReady, etc.) to the control plane (**API server**).
- The **controller manager** has a **Node Controller** that periodically checks node **heartbeats**.
- If the **Node Controller** does not receive heartbeats from a node for `--node-monitor-grace-period` (default 40s), it marks the node as **NotReady**.

### 2. Pod Eviction and Rescheduling
- If the node remains **NotReady** for longer than `--pod-eviction-timeout` (default 5m), the **Node Controller** starts evicting pods from that node.
- For workloads managed by **ReplicaSets/Deployments/StatefulSets**, Kubernetes **automatically reschedules** the pods on **healthy nodes**.
  - Failed naked pods (defined by `Kind: Pod`, scheduled once) are not rescheduled.
  - Failed static nodes (created directly by kubelet from manifests on a node, bypassing scheduler) disappear unless manually recreated.

### 3. Handling Stateful Workloads
- PVs from failed nodes are **reattached** to new nodes if the StorageClass supports **dynamic provisioning** (e.g., `provisioner: kubernetes.io/aws-ebs`).
  - **Local** PVs can**not** be automatically reattached.
- **StatefulSets** ensure stateful pods keep the **same name and PVC**, preserving data across restarts.

### 4. Service and Load Balancing
- The **kube-proxy** and **Service** abstraction automatically update endpoints.
- Failed pods are removed from the Service endpoints list, so **traffic** is routed **only to healthy pods**.

### 5. Higher Availability with Cluster Autoscaler
- Cloud: Cluster Autoscaler can detect permanently failed **nodes** and provision replacements automatically.
- On-premises/bare-metal: **Node** replacement is usually manual, but pods are still rescheduled on surviving nodes.


## Question 7
> Node Failure Alert Implementations

### 1. Prometheus + Alertmanager
- **Prometheus** scrapes metrics from the cluster, including `kube_node_status_condition{condition="Ready",status="false"}`.
- **Alertmanager** (part of Prometheus ecosystem) evaluates alert rules and can send notifications to:
  - Slack
  - Microsoft Teams
  - Email
  - PagerDuty, Opsgenie, etc.

```mermaid
flowchart LR
    subgraph ClusterMetrics[Cluster Node Metrics]
        C1[Node Exporter / kube-state-metrics]
        
    end
    
    subgraph AppMetrics[Application Metrics]
        A1[App Pod]
        A2[Optional Sidecar Exporter]
        A1 --> A2
        
    end

    C1 -->|Metrics HTTP Endpoint| B
    A2 -->|Metrics HTTP Endpoint| B[Prometheus Pod]
    B -->|Firing Alerts| D[Alertmanager Pod]
    D -->|Slack/Email Notifications| E[Engineers]

    B -->|Queries & Dashboards| F[Grafana Pod]
    F -->|Optional Panel Alerts| E
```

<details>
<summary>Example Configuration</summary>

**Prometheus main config (`prometheus.yml`)**
```yaml
global:
  scrape_interval: 15s

# Alerting rules
rule_files:
  - "rules/node_alerts.yaml"   # path to your Prometheus alerting rules

# Alertmanager configuration
alerting:
  alertmanagers:
  - static_configs:
    - targets:
      - 'alertmanager:9093'  # Replace with your Alertmanager service, e.g., 'alertmanager.monitoring.svc.cluster.local:9093' in K8s

# Metrics scraping configuration
scrape_configs:
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'   # Original node address from K8s, e.g., 'ip-10-0-1-5.ec2.internal:10250'
        target_label: __address__
        replacement: '${1}:9100'  # '${1}' captures the original IP/hostname from regex above
                                     # Example: 'ip-10-0-1-5.ec2.internal:9100', where 9100 is Node Exporter port
```

**Prometheus alerting rule (`node_alerts.yaml`)**
```yaml
groups:
- name: NodeAlerts
  rules:
  - alert: NodeDown
    expr: kube_node_status_condition{condition="Ready",status="false"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Node {{ $labels.node }} is down"
      description: "Node {{ $labels.node }} has been NotReady for more than 5 minutes."
```

**Alertmanager config (`alertmanager.yml`)**
```yaml
global:
  resolve_timeout: 5m

route:
  receiver: 'slack-notifications'

receivers:
- name: 'slack-notifications'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/T00000000/B00000000/XXXXXXXXXXXXXXXXXXXXXXXX'  
    # Replace api_url with your Slack Incoming Webhook
    channel: '#k8s-alerts' 
    # Replace channel with the target Slack channel
    text: "{{ .CommonAnnotations.summary }}\n{{ .CommonAnnotations.description }}"
```
</details>

### 2. Cloud Provider Monitoring
- If running on AWS EKS, GCP GKE, or Azure AKS, cloud monitoring services can alert:
  - AWS CloudWatch → email, SNS, Slack, SMS
  - GCP Monitoring → email, Slack, PagerDuty
  - AKS Monitoring → Azure Monitor + alerts
- These services can detect node failures via **instance health checks** or **K8s metrics**.

### 3. Grafana
- **Dashboard-specific or visual-context alerts:** alerts are based on **queries** against a **data source** (Prometheus, InfluxDB, Loki, etc.) that has already collected metrics or events from the cluster.
- Alerts can trigger notifications to:
  - Slack
  - Microsoft Teams
  - Email
  - Webhooks
  - PagerDuty, Opsgenie, etc.


## Question 8
> Internal DNS

### Kubernetes DNS Basics
1. Every Kubernetes cluster includes a DNS service, usually **CoreDNS** in the `kube-system` namespace (or kube-dns in older clusters).
   - CoreDNS **resolves DNS queries** for services and pods within the cluster.

2. Kubernetes automatically creates DNS entries for:

   1. **Services**
      - Format: `<service>.<namespace>.svc.cluster.local`
      - Standard services get a **virtual cluster** that load-balances traffic to matching pods.
   2. **Pods / Headless Services**
      - Optional as pod-to-pod communication can use DNS automatically.
      - Customizable in **pod specs** via `dnsPolicy` and `dnsConfig`
      - For **headless** services (`clusterIP: None`), DNS queries for `<service>.<namespace>.svc.cluster.local` return the **IP addresses of all matching pods** instead of a single cluster IP.
   3. **StatefulSets**
      - Each **pod** gets a **stable, predictable hostname**: `<pod>-<index>.<service>.<namespace>.svc.cluster.local`
      - Works only with **headless** services, allowing direct **pod-level** access.

3. Kubelet Flags Affecting Pod DNS
   - `/etc/resolv.conf` in **pods** is automatically generated based on kubelet flags:
     - `--cluster-dns` – the **internal** IP of the cluster DNS service (usually CoreDNS). Pods use this to resolve service and pod names.
     - `--cluster-domain` — the DNS domain **suffix** for the cluster (default: `cluster.local`).
       - The `.svc.cluster.local` suffix disambiguates internal cluster services from any external domain that might resemble a partially qualified name format (`<service>.<namespace>`).

### Editing CoreDNS ConfigMap
Custom DNS forwarding rules can be defined in the CoreDNS ConfigMap (`kube-system` namespace).
This allows cluster pods to **resolve external or private domains through specified upstream DNS servers**.

<details>
<summary>Example</summary>

```
google.com:53 {  # handle DNS queries for google.com on port 53 (optional default port)
    errors       # log errors
    cache 30     # cache query results for 30 seconds
    forward . 8.8.8.8  # upstream DNS server; dot "." means all subdomains
}

internal.company.local:53 {  # enterprise domain
    forward . 192.168.1.10:5353  # company's interanl DNS server (<private-IP>:<custom-port>)
}
```

```mermaid
flowchart TD
  Pod["Pod\n/etc/resolv.conf\nnameserver: 10.96.0.10"]
  CoreDNS["CoreDNS\n(Cluster DNS service 10.96.0.10)\nResolves:\n- Cluster SVCS\n- Forward rules"]
  ClusterSvc["Cluster Service:\nmy-service.svc.cluster.local"]
  GoogleDNS["Upstream DNS:\ngoogle.com:53"]
  InternalDNS["Upstream DNS:\ninternal.company.local:5353"]

  Pod --> CoreDNS
  CoreDNS --> ClusterSvc
  CoreDNS --> GoogleDNS
  CoreDNS --> InternalDNS

  ClusterSvc .->|ClusterIP| Pod
  GoogleDNS .->|Public IP| Pod
  InternalDNS .->|Private IP| Pod
```
</details>
